\textbf{Third-level Tile Schedule:}
 We implement the third-level tile using an optimized hand-written register-tiled kernel that performs matrix max-plus. We process the third-level register tile in a column-major order ($i_{3}, j_{3} \mapsto {j_3}, i_{3}$).  The design of the register-tiled kernel is target-dependent. We use Intel intrinsic APIs to compute multiple max-plus operations using Intel Advanced Vector Extensions (AVX-256) registers.

Due to many architectural similarities, we use a common register-tiling implementation for our target architectures - Broadwell and Coffee Lake architecture. One of the main differences between these two architectures is the number of available floating-point addition units (FPA) per core. Even though both architectures have two floating-point multiply-add (FMA) units, Broadwell has only one floating-point add unit, whereas Coffee Lake has two floating-point add units. Since the max operation is also executed using the FPA unit and the number of instructions per cycle for vaddps and vmaxps are twice smaller for Broadwell than Coffee Lake,  Broadwell architecture is significantly bottle-necked for the max-plus computation. The objective of the register tiling is to load the data into the registers and perform as many operations as we can without accessing memory. AVX-256 has sixteen 256-bit registers YMM0-YMM15, which perform a single instruction on multiple data elements. Each YMM register can hold eight single-precision floating points and be used to store operands or results to perform eight single-precision operations. The execution latency of vaddps and vmaxps operation on Coffee Lake is four cycles (3 for the Broadwell). Thus, we need to have 8 (6 for the Broadwell) independent chains of computations to fully utilize both FPA execution ports for Coffee Lake.

We are interested in a data access pattern of $C = (a + B) \max C $, where $a$ is a scalar and $B$, $C$ are vectors. So, we load eight consecutive elements (vector) of $B$ and $C$ into the YMM registers (B, C) but load a single element (scalar) of $a$ and broadcast it to a YMM register(A). The goal is to find the combination of $A$ and $B$ that maximizes CPU-resource utilization. Table~\ref{tab:ymm_registers} shows the different register tiles ($ A\times B$) that maximize the resource utilization but minimize the AVX register allocation. We notice that $3 \times 24$ maximizes the resource utilization with the best memory access to compute ratio. 

\textbf{Memory Access:} We implement several techniques to optimize memory access. Each core is responsible for executing a complete matrix max-plus operation that requires data transformation. We use a dedicated buffer for these transformations and select them using omp\_get\_thread\_num(). We ensure the processor-to-memory affinity by setting OMP\_PLACES to 'cores' and set OMP\_PROC\_BIND to true to bind the OMP threads to the physical core to improve data locality during the on-the-fly memory transformation. We use Intel intrinsic for allocating these transformation buffers so that they are aligned to SIMD width ($8 \times 4$ bytes).

